{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter5Deeplearningforcomputervision.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPXK1W6FEb9mhGK/eWkr876",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abernauer/ECON_APPLIED_ML/blob/master/Chapter5Deeplearningforcomputervision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDNebN4mNGZ3",
        "colab_type": "text"
      },
      "source": [
        "#Chapter 5: Deep learning for computer vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx6xaNdzNRys",
        "colab_type": "text"
      },
      "source": [
        "#5.1 Introduction to convnets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AllhzeSONerQ",
        "colab_type": "text"
      },
      "source": [
        "We're about to dive into the theory of what convnets are and why they have been so successful at computer vision tasks. First will revisit the MNIST digit example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqpZehMKM-DO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers \n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D(2, 2))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1pITugW2wP8",
        "colab_type": "text"
      },
      "source": [
        "A convnet takes as input tensors of shape( image_height, image_width, image_channels) (not including the batch dimension). In this case, we'll configure the convnet to process inputs of size (28, 28, 1), which is the format of MNIST images. We did this by passing the argument input_shape=(28, 28, 1) to the first layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INCWgJQc38FB",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the architecture so far:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyfTakHJ4Afv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "e620e964-15fc-4130-fde8-2b24a14c91a6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpRCZNTX4YIW",
        "colab_type": "text"
      },
      "source": [
        "The output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of channels is controlled by the first argument passed to the Conv2D layers (32 or 64).\n",
        "\n",
        "The next step is to feed the last output tensor (of shape (3, 3, 64)) into a densely connected classifier network like those you're already familiar with: a stack of Dense layers. These classifiers process vectors, which are 1D, whereas the current output is a 3D tensor. First we have to flatten the 3D outputs to 1D, and then add a few Dense layers on top."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7VL9tcO5tVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(layers.Flatten())       \n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfXuou98BpW0",
        "colab_type": "text"
      },
      "source": [
        "We'll do 10-way classification, using a final layer with 10 outputs and a sofmax activitaion.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-lXpI1rBp0M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "413823cc-f0ce-46bd-8ec0-54a3b676189e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73qDopLsDvch",
        "colab_type": "text"
      },
      "source": [
        "The (3, 3, 64) outputs are flattened into vectors of shape (576, ) before going through two Dense layers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zmxkREjEnDe",
        "colab_type": "text"
      },
      "source": [
        "Let's train the convnet on the MNIST digits.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND42LdyVEnim",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "c662b0fa-4ca0-46e6-d574-5c5eb1e6ae1e"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(train_images, train_labels),  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 49s 816us/step - loss: 0.1768 - accuracy: 0.9441\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 49s 813us/step - loss: 0.0459 - accuracy: 0.9856\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 48s 799us/step - loss: 0.0320 - accuracy: 0.9901\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 48s 793us/step - loss: 0.0239 - accuracy: 0.9921\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 47s 791us/step - loss: 0.0181 - accuracy: 0.9945\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7fd4820dc240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22Wp-IKzGPTI",
        "colab_type": "text"
      },
      "source": [
        "Let's evaluate the model on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAcTVip7HakT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "75698fbb-4aef-4cc6-fb80-9a21de253d40"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 3s 286us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01szJWPKHklk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4af9c0d2-50b1-49f2-e25d-e9262843cd6b"
      },
      "source": [
        "test_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9916999936103821"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQw28ba0HpXk",
        "colab_type": "text"
      },
      "source": [
        "The basic convnet has a test accuracy of 99.2% which reduced the error rate in comparison the densely connected network from chapter 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60zA-9_IISMK",
        "colab_type": "text"
      },
      "source": [
        "#5.1.1 The convolution operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8EeiPD2JRg-",
        "colab_type": "text"
      },
      "source": [
        "The fundamental difference between a densely connected layer and a convolution layer is this: Dense layers learn global pattern in their input feature space, whereas convolution layers learn local patterns: in the case of images, patterns found in small 2D windows of the inputs. In the previous example, these windows were all 3 x 3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znlSq-3VJ2Rq",
        "colab_type": "text"
      },
      "source": [
        "This key characteristic gives convnets two interesting properties:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcJQFj6jKFj2",
        "colab_type": "text"
      },
      "source": [
        "* *The patterns they learn are translation invariant*. After learning a certain pattern in the lower-rigth corner of a picture, a convnet can recognize it anywhere: for example, in the upper-left corner. A densely connected network would have to learn pattern annew if it appeared at a new location. This makes convnets data efficient when processing images (because *the visual world is fundamentally translation invariant*): they need fewer training samples to learn representations that have generalization power.\n",
        "\n",
        "* *They can learn spatial hierarchies of patterns*. A first convolution layer will learn small local patterns such as edges, a second convolution layer will learn larger patterns made of the features of the first layers, and so on. This allows convnets to efficiently learn increasingly complex and abstract visual concepts (because *the visual world is fundamentally spatially hierarchical*.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVKvLuPJM-j_",
        "colab_type": "text"
      },
      "source": [
        "Convolutions operate over 3D tensors, called *feature maps*, with two spatial axes( *height* and *width*) as well as a *depth* axis (also called the *channels* axis). For an RGB image, the dimension of the depth axis is 3, because the image has three color channels: red, green, and blue. The convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an *output feature map*. This output feature map is still a 3D tensor: it has a width and a height. Its depth can be arbitrary, because the output depth is a parameter of the layer, and the different channels in that depth axis no longer stand for specific colors as in RGB input; rather, they stand for *filters*. Filters encode specific apsects of the input: at a high level, a single filter could encode the concept \"presence of a face in the input,\" for instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR7PeSgkEiVY",
        "colab_type": "text"
      },
      "source": [
        "In the MNIST example, the first convolution layer takes a feature map of size (28, 28, 1) and outputs a feature map of size (26, 26, 32): it computes 32 filters over its input. Each of these 32 output channels contains a 26 X 26 grid of values, which is a *response map* of the filter over the input, indicating the response of that filter pattern at different locations in the input. That is what the term *feature map* means: every dimension in the depth axis is a feature (or filter), and the 2D tensor output[:, :, n] is the 2D spatial *map* of the response of this filter over the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shMB2n6mHiEG",
        "colab_type": "text"
      },
      "source": [
        "Convolutions are defined by two key parameters:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xohUGUcQHoAC",
        "colab_type": "text"
      },
      "source": [
        "* *Size of the patches extracted from the inputs* -- These are typically 3 x 3 or 5 x 5. In the example, they were 3 x 3, which is a common choice.\n",
        "* *Depth of the output feature map* -- The number of filters computed by the convolution. The example started it out with a depth of 32 and ended with a depth of 64."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OyLcPajI3QY",
        "colab_type": "text"
      },
      "source": [
        "In Keras Conv2D layers, these parameters are the first arguments passed to the layer: Conv2D(output_depth, (window_height, window_width))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUezV6xcJy1k",
        "colab_type": "text"
      },
      "source": [
        "A convolution works by *sliding* these windows of size 3 x 3 or 5 x 5 over the 3D input feature map, stopping at every possible location, and extracting the 3D patch of surrounding features. Each such 3D patch is then transformed (via a tensor product with the same learned weigth matrix, called the *convolution kernel*) into a 1D vector of shape (output_depth,). All of these vectors are then spatially reassembled into a 3D output map of shape (height, width, output_depth). Every spatial location in the output feature map corresponds to the same location in the input feature map. For instance, with 3 x 3 windows, the vector output[i, j, :] comes from the 3D patch input[i-1:i+1, j-1:j+1, :]. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z59TPYLSabRV",
        "colab_type": "text"
      },
      "source": [
        "Note that the output width and height may differ from the input width and height. They may differ for two reasons:\n",
        "* Border effects, which can be countered by padding the input feature map\n",
        "* The use of *strides*, which I'll define in a second."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bXpZrmBbEKJ",
        "colab_type": "text"
      },
      "source": [
        "# UNDERSTANDING BORDER EFFECTS AND PADDING "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUBTg3m1bMgY",
        "colab_type": "text"
      },
      "source": [
        "Consider a 5 x 5 feature map (25 tiles total). There are only 9 tiles around which you can center a 3 x 3 window, forming a 3 x 3 grid. Hence, the output feature map will be 3 x 3. It shrinks a little; by exactly two tiles alongside each dimension, in this case. You can see this border effect in action in the earlier example: you start with 28 x 28, which shrinks to 26 x 26 after the first convolution layer.\n",
        "\n",
        "If you want to get an output feature map with the same spatial dimensions as the input, you can use *padding*. Padding consists of adding an appropriate number of rows and columns on each side of the input feature map so as to make it possible to fit center convolution windows around every input tile. For a 3 x 3 window, you add one column on the right, one column on the left, on row at the top, and one row at the bottom. For a 5 x 5 window, you add two rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qK5LNFsYq0OX",
        "colab_type": "text"
      },
      "source": [
        "In Conv2D layers, padding is configurable via the padding argument, whic takes two values: \"valid\", which means no padding (only valid window locations will be used); and \"same\", which means \"pad in such a way as to have an output with the same width and height as the input.\" The padding argument defaults to \"valid\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irHi6keMt9Yq",
        "colab_type": "text"
      },
      "source": [
        "#UNDERSTANDING CONVOLUTION STRIDES"
      ]
    }
  ]
}